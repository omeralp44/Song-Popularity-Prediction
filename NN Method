#NN Implementation


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random

data= pd.read_csv("song_data.csv")

data_matrix1 = data.drop(["song_name"], axis = 1)
data_matrix = np.array(data_matrix1)

def normalization(M): #Rescaling (min-max normalization) (0-1000)
 M1 = ((M - np.min(M)) * 1) / (np.max(M) - np.min(M)) 
 return M1

data_matrix = normalization(data_matrix)

def cross_val_split(X): #5-Fold
 
 A = int(X.shape[0]/5)
 B = 2*A
 C = 3*A
 D = 4*A
 E = 5*A
 
 test_data1 = X[0:A]
 test_data2 = X[A:B]
 test_data3 = X[B:C]
 test_data4 = X[C:D]
 test_data5 = X[D:E] 
 
 data_matrix_A = X 
 for i in range (0,A):
 data_matrix_A = np.delete(data_matrix_A, i, axis = 0) 
 train_data1 = data_matrix_A 
 
 data_matrix_B = X 
 for i in range (A,B):
 data_matrix_B = np.delete(data_matrix_B, i, axis = 0)
 train_data2 = data_matrix_B 
 data_matrix_C = X 
 for i in range (B,C):
 data_matrix_C = np.delete(data_matrix_C, i, axis = 0)
 train_data3 = data_matrix_C
 data_matrix_D = X 
 for i in range (C,D):
 data_matrix_D = np.delete(data_matrix_D, i, axis = 0)
 train_data4 = data_matrix_D
 train_data5 = X[0:D] 
 return test_data1, test_data2, test_data3, test_data4, test_data5, tr
ain_data1, train_data2, train_data3, train_data4, train_data5

test_data1, test_data2, test_data3, test_data4, test_data5, train_data1
, train_data2, train_data3, train_data4, train_data5 = cross_val_split(
data_matrix)

def layer_sizes(X):
 n_x = X.shape[1]
 n_y = 1
 n_h = 3
 return n_x, n_h, n_y

def initialize_parameters(n_x, n_h, n_y):
 
 w0 = 1
 
 b1 = np.random.uniform(-w0,w0, size = (1,n_h))
 b2 = np.random.uniform(-w0,w0, size = (1,n_y))
 W1 = np.random.uniform(-w0,w0, size = (n_x,n_h))
 W2 = np.random.uniform(-w0,w0, size = (n_h,n_y))
 
 parameters = { "W1": W1,
 "b1": b1,
 "W2": W2,
 "b2": b2 }
 
 return parameters

def forward_propagation(X, parameters):
 
 W1 = parameters["W1"]
 W2 = parameters["W2"]
 b1 = parameters["b1"]
 b2 = parameters["b2"]
 
 
 Z1 = np.dot(X,W1) + b1
 A1 = (Z1)
 Z2 = A1.dot(W2) + b2
 A2 = (Z2) 
 
 cache = { "Z1": Z1,
 "A1": A1,
 "Z2": Z2,
 "A2": A2 }
 
 return A2, cache
def calculate_cost(X,Y,cache):
 
 #MSE
 
 N = X.shape[0]
 A2 = cache["A2"]
 cost1 = np.dot(((A2 - Y)).T, (A2 - Y))
 cost = np.sum(cost1, axis = 1) / (N)
 
 return cost

def calculate_cost2(X,Y,cache):
 
 #MSE
 
 N = X.shape[0]
 A2 = cache["A2"]
 cost = (np.square(A2 - Y))
 
 return cost

def backward_propagation(parameters, cache, Y, X):
 N = X.shape[0]
 
 W1 = parameters["W1"]
 W2 = parameters["W2"]
 b1 = parameters["b1"] 
 A1 = cache["A1"]
 A2 = cache["A2"]
 
 db1 = np.sum(((-
2 * Y + 2 * A2) / N) @ (W2.T), axis = 0, keepdims = True)
 db2 = np.sum(((-2 * Y + 2 * A2) / N), axis = 0, keepdims = True)
 
 dW2 = (np.dot(((-2 * Y + 2 * A2) / N).T, (np.dot(X,W1) + b1) )).T
 dW1 = ((((-2 * Y + 2 * A2) / N) @ W2.T).T @ (X)).T
 
 grads = {"dW1": dW1,
 "db1": db1,
 "dW2": dW2,
 "db2": db2}
 
 return grads

def update_parameters(parameters, grads, learning_rate):
 
 W1 = parameters["W1"]
 W2 = parameters["W2"]
 b1 = parameters["b1"]
 b2 = parameters["b2"]
 dW1 = grads["dW1"]
 dW2 = grads["dW2"]
 db1 = grads["db1"]
 db2 = grads["db2"]
 W1 = W1 - learning_rate * dW1
 W2 = W2 - learning_rate * dW2
 b1 = b1 - learning_rate * db1
 b2 = b2 - learning_rate * db2
 
 parameters = {"W1": W1,
 "b1": b1,
 "W2": W2,
 "b2": b2}
 
 return parameters

# NN_model
def nn_model(X, Y, n_h, learning_rate, num_iterations):
 
 n_x = layer_sizes(X)[0]
 n_y = layer_sizes(X)[2]
 plot = []
 
 # Initialize parameters
 parameters = initialize_parameters(n_x, n_h, n_y)
 
 W1 = parameters["W1"]
 b1 = parameters["b1"]
 W2 = parameters["W2"]
 b2 = parameters["b2"]
 
 # Loop (gradient descent)
 for i in range(0, num_iterations + 1): 
 A2, cache = forward_propagation(X, parameters)
 
 cost = calculate_cost(X, Y, cache)
 
 grads = backward_propagation(parameters, cache, Y, X)
 
 parameters = update_parameters(parameters, grads, learning_rate)
 
 plot.append(cost)
 if i % 25 == 0:
 print ("Cost after iteration %i: %f" %(i, cost))
 print("\n") 
 
 return parameters,plot

# NN_model2
def nn_model2(X, Y, n_h, learning_rate, num_iterations):
 
 n_x = layer_sizes(X)[0]
 n_y = layer_sizes(X)[2]
 plot2 = []
 
 # Initialize parameters
 parameters, plot = nn_model(X_train1, Y_train1, 3, 0.003, num_itera
tions= 10)
 
 W1 = parameters["W1"]
 b1 = parameters["b1"]
 W2 = parameters["W2"]
 b2 = parameters["b2"]
 
 # Loop (gradient descent)
 for i in range(0, num_iterations + 1): 
 A2, cache = forward_propagation(X, parameters)
 
 cost = calculate_cost(X, Y, cache)
 
 grads = backward_propagation(parameters, cache, Y, X)
 
 parameters = update_parameters(parameters, grads, learning_rate)
 
 plot2.append(cost)
 if i % 25 == 0:
 print ("Cost after iteration %i: %f" %(i, cost))
 print("\n") 
 
 return parameters,plot2

# NN_model3
def nn_model3(X, Y, n_h, learning_rate, num_iterations):
 
 n_x = layer_sizes(X)[0]
 n_y = layer_sizes(X)[2]
 plot3 = []
 
 # Initialize parameters
 parameters, plot = nn_model2(X_train2, Y_train2, 3, 0.003, num_iter
ations= 10)
 
 W1 = parameters["W1"]
 b1 = parameters["b1"]
 W2 = parameters["W2"]
 b2 = parameters["b2"]
 
 # Loop (gradient descent)
 for i in range(0, num_iterations + 1): 
 A2, cache = forward_propagation(X, parameters)
 
 cost = calculate_cost(X, Y, cache)
 
 grads = backward_propagation(parameters, cache, Y, X)
 
 parameters = update_parameters(parameters, grads, learning_rate)
 
 plot3.append(cost)
 if i % 25 == 0:
 print ("Cost after iteration %i: %f" %(i, cost))
 print("\n") 
 
 return parameters,plot3

# NN_model4
def nn_model4(X, Y, n_h, learning_rate, num_iterations):
 
 n_x = layer_sizes(X)[0]
 n_y = layer_sizes(X)[2]
 plot4 = []
 
 # Initialize parameters
 parameters, plot = nn_model3(X_train3, Y_train3, 3, 0.003, num_iter
ations= 10)
 
 W1 = parameters["W1"]
 b1 = parameters["b1"]
 W2 = parameters["W2"]
 b2 = parameters["b2"]
 
 # Loop (gradient descent)
 for i in range(0, num_iterations + 1): 
 A2, cache = forward_propagation(X, parameters)
 
 cost = calculate_cost(X, Y, cache)
 
 grads = backward_propagation(parameters, cache, Y, X)
 
 parameters = update_parameters(parameters, grads, learning_rate)
 
 plot4.append(cost)
 if i % 25 == 0:
 print ("Cost after iteration %i: %f" %(i, cost))
 print("\n") 
 
 return parameters,plot4

# NN_model5
def nn_model5(X, Y, n_h, learning_rate, num_iterations):
 
 n_x = layer_sizes(X)[0]
 n_y = layer_sizes(X)[2]
 plot5 = []
 
 # Initialize parameters
 parameters, plot = nn_model4(X_train4, Y_train4, 3, 0.003, num_iter
ations= 10)
 
 W1 = parameters["W1"]
 b1 = parameters["b1"]
 W2 = parameters["W2"]
 b2 = parameters["b2"]
 
 # Loop (gradient descent)
 for i in range(0, num_iterations + 1): 
 A2, cache = forward_propagation(X, parameters)
 
 cost = calculate_cost(X, Y, cache)
 
 grads = backward_propagation(parameters, cache, Y, X)
 
 parameters = update_parameters(parameters, grads, learning_rate)
 
 plot5.append(cost)
 if i % 25 == 0:
 print ("Cost after iteration %i: %f" %(i, cost))
 print("\n") 
 
 return parameters,plot5

def cross_val_train_test(test_data, train_data):
 
 Y_train = (train_data[:,0])
 Y_train = Y_train.reshape(len(train_data),1)
 
 X_train = np.delete(train_data, 0, axis = 1)
 Y_test = (test_data[:,0])
 Y_test = Y_test.reshape(len(test_data),1)
 
 X_test = np.delete(test_data, 0, axis = 1)
 return X_train, Y_train, X_test, Y_test

X_train1, Y_train1, X_test1, Y_test1 = cross_val_train_test(test_data1,
train_data1)
X_train2, Y_train2, X_test2, Y_test2 = cross_val_train_test(test_data2,
train_data2)
X_train3, Y_train3, X_test3, Y_test3 = cross_val_train_test(test_data3,
train_data3)
X_train4, Y_train4, X_test4, Y_test4 = cross_val_train_test(test_data4,
train_data4)
X_train5, Y_train5, X_test5, Y_test5 = cross_val_train_test(test_data5,
train_data5)

parameters_test1, plot_MSE_train1 = nn_model(X_train1, Y_train1, 3, 0.0
03, num_iterations= 50)
parameters_test2, plot_MSE_train2 = nn_model2(X_train2, Y_train2, 3, 0.
003, num_iterations= 50)
parameters_test3, plot_MSE_train3 = nn_model3(X_train3, Y_train3, 3, 0.
003, num_iterations= 50)
parameters_test4, plot_MSE_train4 = nn_model4(X_train4, Y_train4, 3, 0.
003, num_iterations= 50)
parameters_test5, plot_MSE_train5 = nn_model5(X_train5, Y_train5, 3, 0.
003, num_iterations= 50)

plt.figure(figsize=(10,10))
plt.plot(plot_MSE_train1)
plt.plot(plot_MSE_train2)
plt.plot(plot_MSE_train3)
plt.plot(plot_MSE_train4)
plt.plot(plot_MSE_train5)
plt.xlabel("Number of iterations")
plt.ylabel("Error")
plt.grid()
plt.legend(['MSE_train1', 'MSE_train2','MSE_train3','MSE_train4','MSE_t
rain5'])

A2_test1, cache_test1 = forward_propagation(X_test1, parameters_test1)
cost_test1 = calculate_cost2(A2_test1, Y_test1, cache_test1)
A2_test2, cache_test2 = forward_propagation(X_test2, parameters_test2)
cost_test2 = calculate_cost2(A2_test2, Y_test2, cache_test2)
A2_test3, cache_test3 = forward_propagation(X_test3, parameters_test3)
cost_test3 = calculate_cost2(A2_test3, Y_test3, cache_test3)
A2_test4, cache_test4 = forward_propagation(X_test4, parameters_test4)
cost_test4 = calculate_cost2(A2_test4, Y_test4, cache_test4)
A2_test5, cache_test5 = forward_propagation(X_test5, parameters_test5)
cost_test5 = calculate_cost2(A2_test5, Y_test5, cache_test5)
total_cost = (cost_test1 + cost_test2 + cost_test3 + cost_test4 + cost_
test5) / 5

plt.figure(figsize=(10,10))
plt.plot(total_cost)
plt.xlabel("Number of iterations")
plt.ylabel("Error")
plt.grid()
plt.legend(['MSE_total'])

def accuracy(y_pred, y_test):
 y_pred = np.mean(y_pred)
 y_test = np.mean(y_test)
 
 acc = (y_pred * 100 / y_test)
 return acc

acc1 = accuracy(A2_test1, Y_test1)
acc2 = accuracy(A2_test2, Y_test2)
acc3 = accuracy(A2_test3, Y_test3)
acc4 = accuracy(A2_test4, Y_test4)
acc5 = accuracy(A2_test5, Y_test5)
acc = (acc1 + acc2 + acc3 + acc4 + acc5) / 5
print("%",acc)
